---
permalink: /
title: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am currently a second-year PhD student in **Computer Science and Technology** at the [Xi’an Research Institute of High Technology], specializing in **large language models (LLMs)**, **machine learning**, and **AI safety evaluation**. My research focuses on developing robust methodologies and automated tools to assess the safety, trustworthiness, and performance of LLMs in real-world applications. I aim to bridge the gap between cutting-edge machine learning technologies and their safe deployment.

### Research Interests:
- **Large Language Models (LLMs)**: My research in LLMs involves enhancing **safety**, **confidence calibration**, and **performance optimization**. I explore how to better align the model's confidence levels with its actual capabilities, using techniques like **cognitive diagnosis** and **semantic mutation** to fine-tune and evaluate model behavior in various contexts.
  
- **Safety Evaluation**: I am working on developing **fine-grained security risk classification** methods for automated safety evaluation of LLMs. My goal is to identify vulnerabilities within LLMs and design scalable evaluation strategies that assess the model’s response to various risks, including bias, misinformation, and adversarial attacks.
  
- **Cognitive Diagnosis**: Cognitive modeling plays a central role in my approach to improving model confidence. I focus on methods that allow models to **self-diagnose** and **self-correct** their responses, improving both **accuracy** and **trustworthiness**. This includes developing approaches for confidence expression through **diversity in question mutation** and **answer consistency**.

- **Model Robustness and Security**: I am dedicated to ensuring that LLMs are **resilient to adversarial attacks** and capable of maintaining **secure performance** under diverse, unpredictable conditions. My research involves building **adversarial testing systems** and using reinforcement learning to simulate attack scenarios, helping to evaluate and improve LLMs’ robustness.

### Current Research Projects:
1. **Confidence Calibration for LLMs**: I am exploring techniques to **calibrate the confidence** of large language models, with a particular focus on enhancing the model's ability to assess and express its confidence level accurately. This work leverages **self-consistency** methods and **answer variation techniques** to quantify the uncertainty in model responses and improve overall trustworthiness.

2. **Automated Safety Evaluation Tools**: I am developing automated tools for **real-time safety evaluation** of LLM-generated content. These tools are designed to assess the **security risks** of generated responses, helping to prevent harmful outputs such as bias, discrimination, or misinformation. My approach focuses on building comprehensive **evaluation frameworks** that can adapt to evolving models and datasets.

3. **Red Teaming and Adversarial Testing**: To evaluate the robustness and security of LLMs, I use **reinforcement learning algorithms** to simulate adversarial attacks. The goal is to identify vulnerabilities and weaknesses in model responses under various adversarial conditions. This project aims to ensure LLMs adhere to ethical guidelines and perform safely in deployment.

4. **Cognitive Diagnosis for Confidence Calibration**: As part of my broader goal of improving model safety, I am researching **cognitive diagnosis** methodologies to assess and improve a model’s confidence in different contexts. This involves quantifying model certainty through a variety of testing and training methods to help align model confidence with the actual performance of the model.


### Future Directions:
- **Ethical AI Development**: A significant focus of my work is ensuring that the development of LLMs is aligned with ethical standards. I plan to contribute to the ongoing development of guidelines and practices that ensure AI systems respect privacy, fairness, and human values.
- **Cross-Disciplinary Research**: In addition to my work on LLMs, I am eager to explore cross-disciplinary approaches that combine machine learning with other fields like cognitive science, neuroscience, and human-computer interaction to advance both theoretical and practical AI safety.

---

### Contact Information:
- **Email**: [nilximme@gmail.com](mailto:nilximme@gmail.com)
- **GitHub**: [https://github.com/bodoa7](https://github.com/bodoa7)

---

